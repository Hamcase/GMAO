# --- IMPORTATIONS STANDARD ET DE BIBLIOTH√àQUES TIERCES ---
import pytesseract
import uvicorn
import os
import re # Pour le d√©coupage (chunking)
from io import BytesIO

from fastapi import FastAPI, UploadFile, File, HTTPException, Depends, Header, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
from fastapi.security import OAuth2PasswordBearer
from jose import JWTError, jwt
from pydantic import BaseModel
from pdf2image import convert_from_bytes
from pypdf import PdfReader
from dotenv import load_dotenv
from pathlib import Path

# --- NOUVELLES IMPORTATIONS POUR RAG ET LLM ---
from supabase import create_client, Client # Pour se connecter √† la DB Supabase (Auth uniquement)
from mistralai import Mistral

# --- IMPORTATIONS DES MODULES RAG ---
from rag.chroma_manager import ChromaManager
from rag.chunking import SmartChunker
from rag.hybrid_search import HybridSearcher
from rag.reranker import CrossEncoderReranker
from rag.citation_tracker import CitationTracker
from rag.ocr_processor import OCRProcessor

# --- CHARGEMENT DES SECRETS DEPUIS backend/.env ---
load_dotenv()

# --- LOCAL PDF STORAGE SETUP ---
PDF_STORAGE_DIR = Path("./pdf_storage")
PDF_STORAGE_DIR.mkdir(exist_ok=True)
print(f"üìÅ PDF Storage directory: {PDF_STORAGE_DIR.absolute()}")

# Secrets pour l'authentification (v√©rifier le token JWT de l'utilisateur)
SUPABASE_JWT_SECRET = os.environ.get("SUPABASE_JWT_SECRET")
ALGORITHM = "HS256"

# Secrets pour la connexion √† la base de donn√©es Supabase (pour lire/√©crire)
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_SERVICE_KEY = os.environ.get("SUPABASE_SERVICE_KEY")

# Secret pour l'API Mistral (pour la g√©n√©ration de r√©ponse)
MISTRAL_API_KEY = os.environ.get("MISTRAL_API_KEY")

# V√©rification que toutes les cl√©s n√©cessaires sont pr√©sentes
if not all([SUPABASE_JWT_SECRET, SUPABASE_URL, SUPABASE_SERVICE_KEY, MISTRAL_API_KEY]):
    raise ValueError("Erreurs de configuration: veuillez v√©rifier le fichier .env (Supabase ET Mistral)")

# --- INITIALISATION DES CLIENTS (Services externes) ---

# 1. Client de base de donn√©es Supabase
try:
    supabase: Client = create_client(SUPABASE_URL, SUPABASE_SERVICE_KEY)
    print("Connexion √† Supabase (Service) r√©ussie.")
except Exception as e:
    print(f"Erreur de connexion √† Supabase: {e}")
    exit(1)

# 2. RAG Pipeline Components
try:
    chroma_manager = ChromaManager(
        persist_directory="./chroma_db",
        embedding_model="all-MiniLM-L6-v2"
    )
    smart_chunker = SmartChunker(chunk_size=800, chunk_overlap=100, language='fr')
    hybrid_searcher = HybridSearcher(alpha=0.5)  # Equal weight semantic + keyword
    reranker = CrossEncoderReranker(model_name="cross-encoder/ms-marco-MiniLM-L-6-v2")
    citation_tracker = CitationTracker()
    ocr_processor = OCRProcessor(languages=['fr', 'en'], gpu=False)
    print("RAG Pipeline initialis√© (ChromaDB + Hybrid Search + Reranker + OCR).")
except Exception as e:
    print(f"Erreur d'initialisation du pipeline RAG: {e}")
    exit(1)

# 3. Client pour l'API Mistral (nouveau SDK)
try:
    mistral_client = Mistral(api_key=MISTRAL_API_KEY)
    print("Client Mistral initialis√©.")
except Exception as e:
    print(f"Erreur d'initialisation du client Mistral: {e}")
    exit(1)
# --- FIN DES INITIALISATIONS ---


# --- CONFIGURATION DE L'APPLICATION FASTAPI ---
app = FastAPI(
    title="GMAO+IA Backend",
    description="API pour l'OCR, le RAG et les pr√©visions de maintenance.",
    version="0.1.0"
)

# Configuration CORS (Cross-Origin Resource Sharing)
# Permet au frontend (localhost:3000) d'appeler ce backend (localhost:8000)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"], # Important : autorise l'en-t√™te "Authorization"
)
# ---

# --- SCH√âMAS DE DONN√âES Pydantic ---
class UserTokenData(BaseModel):
    """ Mod√®le pour les donn√©es d√©cod√©es du token JWT Supabase """
    sub: str # ID de l'utilisateur
    aud: str # Audience (devrait √™tre 'authenticated')

class QueryRequest(BaseModel):
    """ Mod√®le pour la requ√™te de recherche/chat """
    query: str

class ForecastRequest(BaseModel):
    """ Mod√®le pour une requ√™te de pr√©vision PDR """
    historical_data: list  # List of {month: str, quantity: float}
    machine: str
    part_reference: str
    model_type: str  # 'prophet', 'arima', 'sarima', 'lstm'
    horizon: int = 12
    params: dict = {}
    use_mtbf: bool = True  # Enable MTBF-based forecast enhancement
    safety_factor: float = 1.0  # Conservative multiplier (e.g., 1.2 for 24/7 assumption)

class HistoricalDataPoint(BaseModel):
    """ Point de donn√©es historique """
    month: str
    quantity: float
# ---

# --- S√âCURIT√â : AUTHENTIFICATION JWT ---
# oauth2_scheme va chercher le token dans l'en-t√™te "Authorization: Bearer <token>"
oauth2_scheme = OAuth2PasswordBearer(tokenUrl="token") # tokenUrl n'est pas utilis√© ici mais requis

async def get_token_from_request(
    authorization: str = Header(None),
    token: str = Query(None)
) -> str:
    """
    Extrait le token JWT depuis l'en-t√™te Authorization ou le query parameter
    """
    # Try Authorization header first
    if authorization and authorization.startswith("Bearer "):
        return authorization.replace("Bearer ", "")
    
    # Fallback to query parameter (for PDF URLs in iframes)
    if token:
        return token
    
    raise HTTPException(
        status_code=401,
        detail="Not authenticated",
        headers={"WWW-Authenticate": "Bearer"},
    )

async def get_current_user(token: str = Depends(get_token_from_request)) -> UserTokenData:
    """
    D√©pendance FastAPI : D√©code et valide le token JWT fourni par Supabase.
    S'ex√©cute avant chaque route prot√©g√©e qui l'utilise.
    """
    credentials_exception = HTTPException(
        status_code=401,
        detail="Impossible de valider les identifiants",
        headers={"WWW-Authenticate": "Bearer"},
    )
    try:
        payload = jwt.decode(
            token,
            SUPABASE_JWT_SECRET,
            algorithms=[ALGORITHM],
            options={"verify_aud": False} # Simplification pour Supabase ('authenticated')
        )
        user_id: str = payload.get("sub")
        if user_id is None: raise credentials_exception
        aud = payload.get("aud")
        if aud != "authenticated": raise credentials_exception # V√©rification d'audience
        return UserTokenData(sub=user_id, aud=aud)
    except JWTError:
        raise credentials_exception
# ---

# --- FONCTION UTILITAIRE : D√âCOUPAGE (Legacy - kept for compatibility) ---
# Note: Using SmartChunker from rag.chunking for new uploads
def chunk_text(text: str, chunk_size: int = 800, overlap: int = 100) -> list[str]:
    """Legacy chunking function - use SmartChunker for new documents"""
    text = re.sub(r'\s+', ' ', text).strip()
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        if end < len(text):
            last_space = text.rfind(' ', start, end)
            if last_space != -1 and last_space > start:
                 end = last_space
        chunks.append(text[start:end].strip())
        if end < len(text) and text[end] == ' ':
            start = end + 1
        else:
             next_start = start + chunk_size - overlap
             start = max(end, next_start) if end < len(text) else next_start
    chunks = [chunk for chunk in chunks if len(chunk) > 10]
    return chunks
# ---

# --- ROUTES DE L'API ---

@app.get("/")
def read_root():
    """ Point d'entr√©e simple pour v√©rifier que l'API est en ligne. """
    return {"status": "GMAO+IA Backend is running!"}

# --- ENDPOINT D'INGESTION (OCR + VECTORISATION + SAUVEGARDE) ---
@app.post("/api/v1/ocr/upload")
async def ocr_and_ingest_document(
    file: UploadFile = File(...),
    current_user: UserTokenData = Depends(get_current_user) # Route prot√©g√©e
):
    """
    Re√ßoit un PDF, extrait le texte (OCR), le d√©coupe, cr√©e des embeddings,
    et sauvegarde le tout dans Supabase (DB + Storage).
    """
    print(f"Traitement du fichier: {file.filename} pour l'utilisateur: {current_user.sub}")

    if file.content_type != "application/pdf":
        raise HTTPException(status_code=400, detail="Type de fichier invalide. Seuls les PDF sont autoris√©s.")

    file_content = await file.read() # Lire le contenu binaire une seule fois

    # --- 1. EXTRACTION DE TEXTE (OCR Enhanced) ---
    full_text = ""
    pages_text = []  # Store text per page for metadata
    
    try:
        # Try digital extraction first
        pdf_reader = PdfReader(BytesIO(file_content))
        print(f"PDF contient {len(pdf_reader.pages)} pages (Extraction num√©rique).")
        for page_num, page in enumerate(pdf_reader.pages):
            page_text = page.extract_text()
            if page_text and len(page_text.strip()) > 50:
                pages_text.append((page_text, page_num + 1))  # Tuple: (text, page_number)
                full_text += page_text + "\n\n"
        
        # Force OCR if extraction is too short
        if len(full_text.strip()) < 100:
            print("Texte num√©rique minimal. For√ßage de l'OCR am√©lior√©.")
            full_text = ""
            pages_text = []
            raise Exception("Force OCR")
        
        print("Extraction num√©rique r√©ussie.")
    
    except Exception as e:
        print(f"Extraction num√©rique √©chou√©e ({e}). Passage √† l'OCR am√©lior√© avec EasyOCR.")
        try:
            # Convert PDF to images
            images = convert_from_bytes(file_content, dpi=200)
            print(f"Conversion du PDF en {len(images)} images pour l'OCR.")
            
            # Process each page with enhanced OCR
            for page_num, image in enumerate(images):
                print(f"Traitement OCR (page {page_num + 1})...")
                # Convert PIL to numpy
                import numpy as np
                image_np = np.array(image)
                
                # Use OCRProcessor for enhanced extraction
                page_text, confidence = ocr_processor.extract_text_from_pdf_page(
                    image_np,
                    preprocess=True
                )
                
                if page_text:
                    pages_text.append((page_text, page_num + 1))  # Tuple: (text, page_number)
                    full_text += page_text + "\n\n"
                    print(f"Page {page_num + 1}: {len(page_text)} chars, confidence: {confidence:.2f}")
            
            print(f"OCR termin√©. Total: {len(full_text)} caract√®res extraits.")
        
        except Exception as ocr_error:
            print(f"ERREUR OCR: {ocr_error}")
            raise HTTPException(
                status_code=500,
                detail=f"Erreur OCR: {ocr_error}. V√©rifiez les d√©pendances."
            )
    
    if len(full_text.strip()) == 0:
        raise HTTPException(
            status_code=400,
            detail="Aucun texte n'a pu √™tre extrait du document."
        )

    # --- 2. SMART CHUNKING WITH METADATA ---
    print("D√©coupage intelligent avec m√©tadonn√©es...")
    try:
        # Chunk by pages with metadata
        all_chunks_with_metadata = smart_chunker.chunk_by_pages(
            pages=pages_text,
            document_name=file.filename
        )
        
        if not all_chunks_with_metadata:
            raise HTTPException(
                status_code=400,
                detail="Le document n'a pas pu √™tre d√©coup√©."
            )
        
        print(f"D√©coupage termin√©: {len(all_chunks_with_metadata)} chunks avec m√©tadonn√©es.")
    
    except Exception as chunk_error:
        print(f"Erreur d√©coupage: {chunk_error}")
        raise HTTPException(
            status_code=500,
            detail=f"Erreur lors du d√©coupage: {chunk_error}"
        )

    # --- 3. INSERTION DANS CHROMADB ---
    try:
        # Extract chunks and metadata
        chunks = [item["content"] for item in all_chunks_with_metadata]
        metadatas = [item["metadata"] for item in all_chunks_with_metadata]
        
        # Add to ChromaDB (automatic embedding)
        count = chroma_manager.add_documents(
            user_id=current_user.sub,
            chunks=chunks,
            metadatas=metadatas
        )
        
        print(f"‚úÖ {count} chunks index√©s dans ChromaDB pour l'utilisateur {current_user.sub}")
    
    except Exception as db_error:
        print(f"Erreur ChromaDB: {db_error}")
        raise HTTPException(
            status_code=500,
            detail=f"Erreur d'indexation: {db_error}"
        )

    # --- 6. SAUVEGARDE DU PDF EN LOCAL ---
    storage_path = None
    try:
        # Create user directory if needed
        user_dir = PDF_STORAGE_DIR / current_user.sub
        user_dir.mkdir(exist_ok=True)
        
        # Save PDF to local filesystem
        file_path = user_dir / file.filename
        with open(file_path, "wb") as f:
            f.write(file_content)
        
        storage_path = f"{current_user.sub}/{file.filename}"
        print(f"‚úÖ PDF sauvegard√© localement: {file_path}")
        
    except Exception as storage_error:
        print(f"‚ö†Ô∏è Erreur de sauvegarde locale (non-bloquante): {storage_error}")
        print(f"Le PDF n'a pas √©t√© sauvegard√© mais reste index√© dans ChromaDB.")
        # Don't raise - storage is optional, ChromaDB indexing is what matters

    # --- 7. R√âPONSE AU FRONTEND ---
    return {
        "status": "Succ√®s",
        "filename": file.filename,
        "message": f"Document trait√© et index√© avec ChromaDB + OCR am√©lior√©.",
        "chunks_indexed": count,
        "storage_path": storage_path,
        "storage_uploaded": storage_path is not None
    }

# --- ENDPOINT DE RECHERCHE RAG + G√âN√âRATION (HYBRID SEARCH + RERANKING) ---
@app.post("/api/v1/rag/query")
async def rag_query_with_generation(
    request: QueryRequest,
    current_user: UserTokenData = Depends(get_current_user)
):
    """
    Pipeline RAG complet:
    1. Recherche hybride (s√©mantique + mot-cl√©)
    2. Re-ranking avec CrossEncoder
    3. G√©n√©ration avec Mistral
    4. Citations avec positions pr√©cises
    """
    print(f"üîç Requ√™te RAG de {current_user.sub}: {request.query}")

    try:
        # --- 1. RECHERCHE VECTORIELLE (CHROMADB) ---
        print("1Ô∏è‚É£ Recherche vectorielle ChromaDB...")
        vector_results = chroma_manager.query(
            user_id=current_user.sub,
            query_text=request.query,
            n_results=10  # Fetch more for reranking
        )
        
        if not vector_results['documents'][0]:
            print("‚ùå Aucun document trouv√© dans ChromaDB")
            return {
                "answer": "D√©sol√©, je n'ai trouv√© aucune information pertinente dans les documents index√©s.",
                "sources": [],
                "citations": []
            }
        
        print(f"‚úÖ Trouv√© {len(vector_results['documents'][0])} r√©sultats vectoriels")

        # --- 2. RECHERCHE HYBRIDE (BM25 + VECTOR FUSION) ---
        print("2Ô∏è‚É£ Fusion hybride (s√©mantique + mot-cl√©)...")
        hybrid_results = hybrid_searcher.hybrid_search(
            query=request.query,
            vector_results=vector_results,
            top_k=10
        )
        print(f"‚úÖ {len(hybrid_results)} r√©sultats fusionn√©s")

        # --- 3. RE-RANKING AVEC CROSSENCODER ---
        print("3Ô∏è‚É£ Re-ranking avec CrossEncoder...")
        reranked_results = reranker.rerank(
            query=request.query,
            candidates=hybrid_results,
            top_k=5  # Keep top 5 for context
        )
        print(f"‚úÖ {len(reranked_results)} r√©sultats re-class√©s")

        if not reranked_results:
            return {
                "answer": "Aucun r√©sultat pertinent apr√®s re-ranking.",
                "sources": [],
                "citations": []
            }

        # --- 4. CONSTRUCTION DU CONTEXTE POUR LE LLM ---
        context_parts = []
        for i, result in enumerate(reranked_results, 1):
            doc_name = result['metadata'].get('document_name', 'Unknown')
            page_num = result['metadata'].get('page_number', 0)
            content = result['document']
            context_parts.append(f"[{i}] Source: {doc_name} (page {page_num})\n{content}")
        
        context = "\n\n---\n\n".join(context_parts)

        # --- 5. G√âN√âRATION AVEC MISTRAL ---
        print("4Ô∏è‚É£ G√©n√©ration de la r√©ponse avec Mistral...")
        system_prompt = """Tu es un assistant expert en maintenance industrielle. 
R√©ponds √† la question en te basant STRICTEMENT sur le contexte fourni. 
Cite tes sources en utilisant [1], [2], etc. qui correspondent aux num√©ros dans le contexte.
Sois concis et pr√©cis. Si l'information n'est pas dans le contexte, dis-le clairement."""

        user_prompt = f"Contexte:\n{context}\n\nQuestion: {request.query}\n\nR√©ponse:"

        try:
            chat_response = mistral_client.chat.complete(
                model="mistral-small-latest",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                temperature=0.2,
                max_tokens=500,
            )

            generated_answer = ""
            if chat_response and getattr(chat_response, "choices", None):
                msg = chat_response.choices[0].message.content
                if isinstance(msg, str):
                    generated_answer = msg
                elif isinstance(msg, list):
                    parts = []
                    for item in msg:
                        if isinstance(item, str):
                            parts.append(item)
                        elif isinstance(item, dict) and item.get("text"):
                            parts.append(item.get("text"))
                        else:
                            parts.append(str(item))
                    generated_answer = " ".join(parts)
                else:
                    generated_answer = str(msg)

            print("‚úÖ R√©ponse g√©n√©r√©e par Mistral")
        
        except Exception as mistral_error:
            print(f"‚ùå Erreur Mistral: {mistral_error}")
            raise HTTPException(
                status_code=502,
                detail=f"Erreur de g√©n√©ration: {mistral_error}"
            )

        # --- 6. TRAITEMENT DES CITATIONS AVEC POSITIONS PR√âCISES ---
        print("5Ô∏è‚É£ Extraction des citations...")
        citations = citation_tracker.create_citation_objects(
            cited_chunks=reranked_results,
            response_text=generated_answer
        )
        
        print(f"‚úÖ {len(citations)} citations extraites")

        # --- 7. FORMATER LA R√âPONSE COMPL√àTE ---
        # Sources pour compatibilit√© (format simplifi√©)
        sources_for_frontend = [
            {
                "document_name": c["document_name"],
                "page_number": c["page_number"],
                "content_preview": c["text"][:150] + "..."
            }
            for c in citations
        ]

        return {
            "answer": generated_answer.strip(),
            "sources": sources_for_frontend,  # Format legacy
            "citations": citations,  # Format enrichi avec char_start/char_end
            "citation_count": len(citations),
            "search_stats": {
                "vector_results": len(vector_results['documents'][0]),
                "hybrid_results": len(hybrid_results),
                "reranked_results": len(reranked_results)
            }
        }

    except HTTPException as http_exc:
        raise http_exc
    except Exception as e:
        print(f"‚ùå Erreur RAG: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(
            status_code=500,
            detail=f"Erreur interne: {str(e)}"
        )

# --- ENDPOINT DE PR√âVISION PDR (MACHINE LEARNING) ---
@app.post("/api/v1/pdr/forecast")
async def forecast_pdr_endpoint(
    request: ForecastRequest,
    current_user: UserTokenData = Depends(get_current_user)
):
    """
    Entra√Æne un mod√®le ML (Prophet, ARIMA, LSTM) sur les donn√©es historiques
    et renvoie les pr√©visions de consommation de pi√®ces.
    Les donn√©es historiques sont envoy√©es directement depuis le frontend (IndexedDB).
    """
    print(f"Requ√™te de pr√©vision PDR de {current_user.sub}: Machine={request.machine}, Pi√®ce={request.part_reference}, Mod√®le={request.model_type}")
    
    try:
        # Import du module de forecasting
        from scripts.forecast_pdr import forecast_pdr
        
        # --- 1. V√©rifier que les donn√©es historiques sont fournies ---
        if not hasattr(request, 'historical_data') or not request.historical_data:
            raise HTTPException(
                status_code=400,
                detail="Aucune donn√©e historique fournie. Veuillez envoyer le champ 'historical_data'."
            )
        
        historical_data = request.historical_data
        print(f"üìä Re√ßu {len(historical_data)} mois de donn√©es historiques depuis le frontend.")
        
        # --- 2. Appeler la fonction de forecasting ---
        print(f"ü§ñ Entra√Ænement du mod√®le {request.model_type}...")
        print(f"‚öôÔ∏è MTBF activ√©: {request.use_mtbf}, Facteur de s√©curit√©: {request.safety_factor}x")
        
        result = forecast_pdr(
            historical_data=historical_data,
            model_type=request.model_type,
            horizon=request.horizon,
            params=request.params,
            use_mtbf=request.use_mtbf,
            safety_factor=request.safety_factor
        )
        
        print(f"‚úÖ Pr√©vision r√©ussie. M√©triques: MAE={result['metrics']['mae']}, R¬≤={result['metrics']['r2']}")
        
        # --- 3. Enrichir avec les m√©tadonn√©es de la requ√™te ---
        result["machine"] = request.machine
        result["part_reference"] = request.part_reference
        result["user_id"] = current_user.sub
        
        return result
        
    except HTTPException as http_exc:
        raise http_exc
    except ImportError as import_err:
        print(f"Erreur d'importation: {import_err}")
        raise HTTPException(
            status_code=500,
            detail=f"Module de pr√©vision non disponible. Installez les d√©pendances: {import_err}"
        )
    except Exception as e:
        print(f"Erreur lors de la pr√©vision PDR: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(
            status_code=500,
            detail=f"Erreur lors de la g√©n√©ration des pr√©visions: {str(e)}"
        )

# --- ENDPOINT POUR SERVIR LES PDFs LOCAUX ---
@app.get("/api/v1/pdf/{user_id}/{filename}")
async def serve_pdf(
    user_id: str,
    filename: str,
    current_user: UserTokenData = Depends(get_current_user)
):
    """
    Sert un PDF stock√© localement
    V√©rifie que l'utilisateur a acc√®s √† son propre fichier
    """
    # V√©rification de s√©curit√©: l'utilisateur ne peut acc√©der qu'√† ses propres fichiers
    if current_user.sub != user_id:
        raise HTTPException(
            status_code=403,
            detail="Acc√®s refus√©: vous ne pouvez acc√©der qu'√† vos propres documents"
        )
    
    # Construire le chemin du fichier
    file_path = PDF_STORAGE_DIR / user_id / filename
    
    # V√©rifier que le fichier existe
    if not file_path.exists() or not file_path.is_file():
        raise HTTPException(
            status_code=404,
            detail=f"Fichier non trouv√©: {filename}"
        )
    
    # Retourner le fichier avec inline disposition (prevents download)
    return FileResponse(
        path=str(file_path),
        media_type="application/pdf",
        headers={
            "Content-Disposition": f"inline; filename={filename}"
        }
    )

# --- Lancement du serveur Uvicorn ---
if __name__ == "__main__":
    # S'ex√©cute quand on lance 'python main.py'
    # Utilise le port 8000 par d√©faut et active le rechargement automatique (--reload)
    uvicorn.run("main:app", host="127.0.0.1", port=8000, reload=True)